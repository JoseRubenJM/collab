{"ast":null,"code":"import _asyncToGenerator from \"/Users/jose/Desktop/proyects/fluid-angular/FluidAngular/collaborative-textarea/node_modules/@babel/runtime/helpers/esm/asyncToGenerator.js\";\n/*!\n * Copyright (c) Microsoft Corporation and contributors. All rights reserved.\n * Licensed under the MIT License.\n */\nimport { SummaryType } from \"@fluidframework/protocol-definitions\";\nimport { assert, bufferToString, stringToBuffer, unreachableCase, fromUtf8ToBase64, Uint8ArrayToString } from \"@fluidframework/common-utils\";\nimport { loggerToMonitoringContext } from \"@fluidframework/telemetry-utils\";\n/*\n * Work around for bufferToString having a bug - it can't consume IsoBuffer!\n * To be removed once bufferToString is fixed!\n*/\nfunction bufferToString2(blob, encoding) {\n  if (blob instanceof Uint8Array) {\n    // IsoBuffer does not have ctor, so it's not in proto chain :(\n    return Uint8ArrayToString(blob, encoding);\n  }\n  return bufferToString(blob, encoding);\n}\n/**\n * Class responsible for aggregating smaller blobs into one and unpacking it later on.\n */\nclass BlobAggregator {\n  constructor() {\n    this.content = [];\n  }\n  addBlob(key, content) {\n    this.content.push([key, content]);\n  }\n  getAggregatedBlobContent() {\n    if (this.content.length === 0) {\n      return undefined;\n    }\n    return JSON.stringify(this.content);\n  }\n  static load(input) {\n    const data = bufferToString2(input, \"utf-8\");\n    return JSON.parse(data);\n  }\n}\n/*\n * Base class that deals with unpacking snapshots (in place) containing aggregated blobs\n * It relies on abstract methods for reads and storing unpacked blobs.\n */\nexport class SnapshotExtractor {\n  constructor() {\n    this.aggregatedBlobName = \"__big\";\n    this.virtualIdPrefix = \"__\";\n    // counter for generation of virtual storage IDs\n    this.virtualIdCounter = 0;\n  }\n  getNextVirtualId() {\n    return `${this.virtualIdPrefix}${++this.virtualIdCounter}`;\n  }\n  unpackSnapshotCore(snapshot, level = 0) {\n    var _this = this;\n    return _asyncToGenerator(function* () {\n      for (const key of Object.keys(snapshot.trees)) {\n        const obj = snapshot.trees[key];\n        yield _this.unpackSnapshotCore(obj, level + 1);\n      }\n      // For future proof, we will support multiple aggregated blobs with any name\n      // that starts with this.aggregatedBlobName\n      for (const key of Object.keys(snapshot.blobs)) {\n        if (!key.startsWith(_this.aggregatedBlobName)) {\n          continue;\n        }\n        const blobId = snapshot.blobs[key];\n        if (blobId !== undefined) {\n          const blob = yield _this.getBlob(blobId, snapshot);\n          for (const [path, value] of BlobAggregator.load(blob)) {\n            const id = _this.getNextVirtualId();\n            _this.setBlob(id, snapshot, value);\n            const pathSplit = path.split(\"/\");\n            let subTree = snapshot;\n            for (const subPath of pathSplit.slice(0, pathSplit.length - 1)) {\n              if (subTree.trees[subPath] === undefined) {\n                subTree.trees[subPath] = {\n                  blobs: {},\n                  trees: {}\n                };\n              }\n              subTree = subTree.trees[subPath];\n            }\n            const blobName = pathSplit[pathSplit.length - 1];\n            assert(subTree.blobs[blobName] === undefined, 0x0f6 /* \"real blob ID exists\" */);\n            subTree.blobs[blobName] = id;\n          }\n          // eslint-disable-next-line @typescript-eslint/no-dynamic-delete\n          delete snapshot.blobs[_this.aggregatedBlobName];\n        }\n      }\n    })();\n  }\n}\n/*\n * Snapshot extractor class that works in place, i.e. patches snapshot that has\n * blob content in ISnapshotTree.blobs itself, not in storage.\n * As result, it implements reading and writing of blobs to/from snapshot itself.\n * It follows existing pattern that mixes concerns - ISnapshotTree.blobs is used for two\n * purposes:\n * 1. map path name to blob ID\n * 2. map blob ID to blob content\n * #2 is what storage (IDocumentStorageService) is for, but in places where we do not have it\n * (like loading serialized earlier draft content), blob content is put directly into snapshot.\n * Ideally this should be fixed by using BlobCacheStorageService or something similar and\n * fixing existing flows to allow switching of storage.\n */\nclass SnapshotExtractorInPlace extends SnapshotExtractor {\n  getBlob(id, tree) {\n    return _asyncToGenerator(function* () {\n      const blob = tree.blobs[id];\n      assert(blob !== undefined, 0x0f7 /* \"aggregate blob missing\" */);\n      return stringToBuffer(blob, \"base64\");\n    })();\n  }\n  setBlob(id, tree, content) {\n    assert(tree.blobs[id] === undefined, 0x0f8 /* \"blob from aggregate blob exists on its own\" */);\n    tree.blobs[id] = fromUtf8ToBase64(content);\n  }\n}\n/*\n * Snapshot packer and extractor.\n * When summary is written it will find and aggregate small blobs into bigger blobs\n * When snapshot is read, it will unpack aggregated blobs and provide them transparently to caller.\n */\nexport let BlobAggregationStorage = /*#__PURE__*/(() => {\n  class BlobAggregationStorage extends SnapshotExtractor {\n    constructor(storage, logger, allowPacking, packingLevel, blobCutOffSize) {\n      super();\n      this.storage = storage;\n      this.logger = logger;\n      this.allowPacking = allowPacking;\n      this.packingLevel = packingLevel;\n      this.blobCutOffSize = blobCutOffSize;\n      this.loadedFromSummary = false;\n      this.virtualBlobs = new Map();\n    }\n    static wrap(storage, logger, allowPacking, packingLevel = 2) {\n      var _a, _b, _c;\n      if (storage instanceof BlobAggregationStorage) {\n        return storage;\n      }\n      const mc = loggerToMonitoringContext(logger);\n      const realAllowPackaging = (_b = (_a = mc.config.getBoolean(\"FluidAggregateBlobs\")) !== null && _a !== void 0 ? _a : allowPacking) !== null && _b !== void 0 ? _b : false;\n      // Always create BlobAggregationStorage even if storage is not asking for packing.\n      // This is mostly to avoid cases where future changes in policy would result in inability to\n      // load old files that were created with aggregation on.\n      const minBlobSize = (_c = storage.policies) === null || _c === void 0 ? void 0 : _c.minBlobSize;\n      return new BlobAggregationStorage(storage, logger, realAllowPackaging, packingLevel, minBlobSize);\n    }\n    static unpackSnapshot(snapshot) {\n      return _asyncToGenerator(function* () {\n        const converter = new SnapshotExtractorInPlace();\n        yield converter.unpackSnapshotCore(snapshot);\n      })();\n    }\n    get policies() {\n      const policies = this.storage.policies;\n      if (policies) {\n        return Object.assign(Object.assign({}, policies), {\n          minBlobSize: undefined\n        });\n      }\n    }\n    unpackSnapshot(snapshot) {\n      var _this2 = this;\n      return _asyncToGenerator(function* () {\n        // SummarizerNodeWithGC.refreshLatestSummary can call it when this.loadedFromSummary === false\n        // (I assumed after file was created)\n        // assert(!this.loadedFromSummary, \"unpack without summary\");\n        _this2.loadedFromSummary = true;\n        yield _this2.unpackSnapshotCore(snapshot);\n      })();\n    }\n    setBlob(id, tree, content) {\n      this.virtualBlobs.set(id, stringToBuffer(content, \"utf-8\"));\n    }\n    getBlob(id, tree) {\n      var _this3 = this;\n      return _asyncToGenerator(function* () {\n        return _this3.readBlob(id).catch(error => {\n          _this3.logger.sendErrorEvent({\n            eventName: \"BlobDedupNoAggregateBlob\"\n          }, error);\n          throw error;\n        });\n      })();\n    }\n    get repositoryUrl() {\n      return this.storage.repositoryUrl;\n    }\n    getVersions(versionId, count) {\n      var _this4 = this;\n      return _asyncToGenerator(function* () {\n        return _this4.storage.getVersions(versionId, count);\n      })();\n    }\n    downloadSummary(handle) {\n      return _asyncToGenerator(function* () {\n        throw new Error(\"NYI\");\n      })();\n    }\n    // for now we are not optimizing these blobs, with assumption that this API is used only\n    // for big blobs (images)\n    createBlob(file) {\n      var _this5 = this;\n      return _asyncToGenerator(function* () {\n        return _this5.storage.createBlob(file);\n      })();\n    }\n    getSnapshotTree(version) {\n      var _this6 = this;\n      return _asyncToGenerator(function* () {\n        const tree = yield _this6.storage.getSnapshotTree(version);\n        if (tree) {\n          yield _this6.unpackSnapshot(tree);\n        }\n        return tree;\n      })();\n    }\n    readBlob(id) {\n      var _this7 = this;\n      return _asyncToGenerator(function* () {\n        if (_this7.isRealStorageId(id)) {\n          return _this7.storage.readBlob(id);\n        }\n        // We support only reading blobs from the summary we loaded from.\n        // This may need to be extended to any general summary in the future as runtime usage pattern\n        // of storage changes (for example, data stores start to load from recent summary, not from original\n        // summary whole container loaded from)\n        // are there other ways we can get here? createFile is one flow, but we should not be reading blobs\n        // in such flow\n        assert(_this7.loadedFromSummary, 0x0f9 /* \"never read summary\" */);\n        const blob = _this7.virtualBlobs.get(id);\n        assert(blob !== undefined, 0x0fa /* \"virtual blob not found\" */);\n        return blob;\n      })();\n    }\n    uploadSummaryWithContext(summary, context) {\n      var _this8 = this;\n      return _asyncToGenerator(function* () {\n        const summaryNew = _this8.allowPacking ? yield _this8.compressSmallBlobs(summary) : summary;\n        return _this8.storage.uploadSummaryWithContext(summaryNew, context);\n      })();\n    }\n    // For simplification, we assume that\n    // - blob aggregation is done at data store level only for now\n    // - data store either reuses previous summary, or generates full summary, i.e. there is no partial (some DDS)\n    // summary produced by data stores.\n    // These simplifications allow us not to touch handles, as they are self-contained (either do not use aggregated\n    // blob Or contain aggregated blob that stays relevant for that sub-tree)\n    // Note:\n    // From perf perspective, it makes sense to place aggregated blobs one level up in the tree not to create extra\n    // tree nodes (i.e. have shallow tree with less edges). But that creates problems with reusability of trees at\n    // incremental summary time - we would need to understand handles and parse them. In current design we can skip\n    // that step because if data store is reused, the hole sub-tree is reused included aggregated blob embedded into it\n    // and that means we can do nothing and be correct!\n    compressSmallBlobs(summary, path = \"\", level = 0, aggregatorArg) {\n      var _this9 = this;\n      return _asyncToGenerator(function* () {\n        if (_this9.blobCutOffSize === undefined || _this9.blobCutOffSize < 0) {\n          return summary;\n        }\n        let shouldCompress = false;\n        let aggregator = aggregatorArg;\n        // checking if this is a dataStore tree, since we only pack at data store level\n        if (Object.keys(summary.tree).includes(\".component\")) {\n          assert(aggregator === undefined, 0x0fb /* \"logic err with aggregator\" */);\n          assert(level === _this9.packingLevel, 0x23b /* \"we are not packing at the right level\" */);\n          aggregator = new BlobAggregator();\n          shouldCompress = true;\n        } else {\n          assert(level !== _this9.packingLevel, 0x23c /* \"we are not packing at the right level\" */);\n        }\n\n        const newSummary = Object.assign({}, summary);\n        newSummary.tree = Object.assign({}, newSummary.tree);\n        for (const key of Object.keys(summary.tree)) {\n          const obj = summary.tree[key];\n          // Get path relative to root of data store (where we do aggregation)\n          const newPath = shouldCompress ? key : `${path}/${key}`;\n          switch (obj.type) {\n            case SummaryType.Tree:\n              // If client created empty tree, keep it as is\n              // Also do not package search blobs - they are part of storage contract\n              if (obj.tree !== {} && key !== \"__search\") {\n                const tree = yield _this9.compressSmallBlobs(obj, newPath, level + 1, aggregator);\n                newSummary.tree[key] = tree;\n                if (tree.tree === {}) {\n                  // eslint-disable-next-line @typescript-eslint/no-dynamic-delete\n                  delete newSummary.tree[key];\n                }\n              }\n              break;\n            case SummaryType.Blob:\n              if (aggregator && typeof obj.content == \"string\" && obj.content.length < _this9.blobCutOffSize) {\n                aggregator.addBlob(newPath, obj.content);\n                // eslint-disable-next-line @typescript-eslint/no-dynamic-delete\n                delete newSummary.tree[key];\n              }\n              break;\n            case SummaryType.Handle:\n              {\n                // Would be nice to:\n                // Trees: expand the tree\n                // Blobs: parse handle and ensure it points to real blob, not virtual blob.\n                // We can avoid it for now given data store is the granularity of incremental summaries.\n                let handlePath = obj.handle;\n                if (handlePath.startsWith(\"/\")) {\n                  handlePath = handlePath.substr(1);\n                }\n                // Ensure only whole data stores can be reused, no reusing at deeper level!\n                assert(level === 0, 0x0fc /* \"tree reuse at lower level\" */);\n                assert(!handlePath.includes(\"/\"), 0x0fd /* \"data stores are writing incremental summaries!\" */);\n                break;\n              }\n            case SummaryType.Attachment:\n              assert(_this9.isRealStorageId(obj.id), 0x0fe /* \"attachment is aggregate blob\" */);\n              break;\n            default:\n              unreachableCase(obj, `Unknown type: ${obj.type}`);\n          }\n        }\n        assert(newSummary.tree[_this9.aggregatedBlobName] === undefined, 0x0ff /* \"duplicate aggregate blob\" */);\n        if (shouldCompress) {\n          // Note: It would be great to add code here to unpack aggregate blob back to normal blobs\n          // If only one blob made it into aggregate. Currently that does not happen as we always have\n          // at least one .component blob and at least one DDS that has .attributes blob, so it's not an issue.\n          // But it's possible that in future that would be great addition!\n          // Good news - it's backward compatible change.\n          assert(aggregator !== undefined, 0x100 /* \"logic error\" */);\n          const content = aggregator.getAggregatedBlobContent();\n          if (content !== undefined) {\n            newSummary.tree[_this9.aggregatedBlobName] = {\n              type: SummaryType.Blob,\n              content\n            };\n          }\n        }\n        return newSummary;\n      })();\n    }\n    isRealStorageId(id) {\n      return !id.startsWith(this.virtualIdPrefix);\n    }\n  }\n  // Tells data store if it can use incremental summary (i.e. reuse DDSes from previous summary\n  // when only one DDS changed).\n  // The answer has to be know long before we enable actual packing. The reason for the is the following:\n  // A the moment when we enable packing, we should assume that all clients out there wil already have bits\n  // that can unpack properly (i.e. enough time passed since we deployed bits that can unpack)\n  // But we can still have clients where some of them already pack, and some do not. If one summary was\n  // using packing, then it relies on non-incremental summaries going forward, even if next client who\n  // produced summary is not packing!\n  // This can have slight improvement by enabling it per file (based on \"did summary we loaded from contain\n  // aggregated blobs\"), but that's harder to make reliable, so going for simplicity.\n  //# sourceMappingURL=blobAggregationStorage.js.map\n  BlobAggregationStorage.fullDataStoreSummaries = true;\n  return BlobAggregationStorage;\n})();","map":null,"metadata":{},"sourceType":"module","externalDependencies":[]}