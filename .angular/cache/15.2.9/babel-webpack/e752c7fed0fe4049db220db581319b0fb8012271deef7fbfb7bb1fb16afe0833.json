{"ast":null,"code":"import _asyncToGenerator from \"/Users/jose/Desktop/proyects/fluid-angular/FluidAngular/collaborative-textarea/node_modules/@babel/runtime/helpers/esm/asyncToGenerator.js\";\n/*!\n * Copyright (c) Microsoft Corporation. All rights reserved.\n * Licensed under the MIT License.\n */\nimport { assert, fromBase64ToUtf8 } from \"@fluidframework/common-utils\";\nimport { ChildLogger } from \"@fluidframework/telemetry-utils\";\nimport { FileMode, TreeEntry } from \"@fluidframework/protocol-definitions\";\nimport { UnassignedSequenceNumber } from \"./constants\";\nimport * as Properties from \"./properties\";\nimport { toLatestVersion, serializeAsMaxSupportedVersion } from \"./snapshotChunks\";\nimport { SnapshotLegacy } from \"./snapshotlegacy\";\nexport let SnapshotV1 = /*#__PURE__*/(() => {\n  class SnapshotV1 {\n    constructor(mergeTree, logger, filename, onCompletion) {\n      var _a, _b, _c;\n      this.mergeTree = mergeTree;\n      this.filename = filename;\n      this.onCompletion = onCompletion;\n      this.logger = ChildLogger.create(logger, \"Snapshot\");\n      this.chunkSize = (_c = (_b = (_a = mergeTree) === null || _a === void 0 ? void 0 : _a.options) === null || _b === void 0 ? void 0 : _b.mergeTreeSnapshotChunkSize, _c !== null && _c !== void 0 ? _c : SnapshotV1.chunkSize);\n    }\n    getSeqLengthSegs(allSegments, allLengths, approxSequenceLength, startIndex = 0) {\n      const segments = [];\n      let length = 0;\n      let segmentCount = 0;\n      while (length < approxSequenceLength && startIndex + segmentCount < allSegments.length) {\n        const pseg = allSegments[startIndex + segmentCount];\n        segments.push(pseg);\n        length += allLengths[startIndex + segmentCount];\n        segmentCount++;\n      }\n      return {\n        version: \"1\",\n        segmentCount,\n        length,\n        segments,\n        startIndex,\n        headerMetadata: undefined\n      };\n    }\n    /**\n     * Emits the snapshot to an ITree. If provided the optional IFluidSerializer will be used when serializing\n     * the summary data rather than JSON.stringify.\n     */\n    emit(serializer, bind) {\n      const chunks = [];\n      this.header.totalSegmentCount = 0;\n      this.header.totalLength = 0;\n      do {\n        const chunk = this.getSeqLengthSegs(this.segments, this.segmentLengths, this.chunkSize, this.header.totalSegmentCount);\n        chunks.push(chunk);\n        this.header.totalSegmentCount += chunk.segmentCount;\n        this.header.totalLength += chunk.length;\n      } while (this.header.totalSegmentCount < this.segments.length);\n      const headerChunk = chunks.shift();\n      headerChunk.headerMetadata = this.header;\n      headerChunk.headerMetadata.orderedChunkMetadata = [{\n        id: SnapshotLegacy.header\n      }];\n      const entries = chunks.map((chunk, index) => {\n        const id = `${SnapshotLegacy.body}_${index}`;\n        this.header.orderedChunkMetadata.push({\n          id\n        });\n        return {\n          mode: FileMode.File,\n          path: id,\n          type: TreeEntry.Blob,\n          value: {\n            contents: serializeAsMaxSupportedVersion(id, chunk, this.logger, this.mergeTree.options, serializer, bind),\n            encoding: \"utf-8\"\n          }\n        };\n      });\n      const tree = {\n        entries: [{\n          mode: FileMode.File,\n          path: SnapshotLegacy.header,\n          type: TreeEntry.Blob,\n          value: {\n            contents: serializeAsMaxSupportedVersion(SnapshotLegacy.header, headerChunk, this.logger, this.mergeTree.options, serializer, bind),\n            encoding: \"utf-8\"\n          }\n        }, ...entries],\n        id: null\n      };\n      return tree;\n    }\n    extractSync() {\n      const mergeTree = this.mergeTree;\n      const {\n        currentSeq,\n        minSeq\n      } = mergeTree.getCollabWindow();\n      this.header = {\n        minSequenceNumber: minSeq,\n        sequenceNumber: currentSeq,\n        orderedChunkMetadata: [],\n        totalLength: 0,\n        totalSegmentCount: 0\n      };\n      this.segments = [];\n      this.segmentLengths = [];\n      // Helper to add the given `MergeTreeChunkV0SegmentSpec` to the snapshot.\n      const pushSegRaw = (json, length) => {\n        this.segments.push(json);\n        this.segmentLengths.push(length);\n      };\n      // Helper to serialize the given `segment` and add it to the snapshot (if a segment is provided).\n      const pushSeg = segment => {\n        if (segment) {\n          pushSegRaw(segment.toJSONObject(), segment.cachedLength);\n        }\n      };\n      let prev;\n      const extractSegment = segment => {\n        // Elide segments that do not need to be included in the snapshot.  A segment may be elided if\n        // either condition is true:\n        //   a) The segment has not yet been ACKed.  We do not need to snapshot unACKed segments because\n        //      there is a pending insert op that will deliver the segment on reconnection.\n        //   b) The segment was removed at or below the MSN.  Pending ops can no longer reference this\n        //      segment, and therefore we can discard it.\n        if (segment.seq === UnassignedSequenceNumber || segment.removedSeq <= minSeq) {\n          return true;\n        }\n        // Next determine if the snapshot needs to preserve information required for merging the segment\n        // (seq, client, etc.)  This information is only needed if the segment is above the MSN (and doesn't\n        // have a pending remove.)\n        if (segment.seq <= minSeq // Segment is below the MSN, and...\n        && (segment.removedSeq === undefined // .. Segment has not been removed, or...\n        || segment.removedSeq === UnassignedSequenceNumber) // .. Removal op to be delivered on reconnect\n        ) {\n          // This segment is below the MSN, which means that future ops will not reference it.  Attempt to\n          // coalesce the new segment with the previous (if any).\n          if (!prev) {\n            // We do not have a previous candidate for coalescing.  Make the current segment the new candidate.\n            prev = segment;\n          } else if (prev.canAppend(segment) && Properties.matchProperties(prev.properties, segment.properties)) {\n            // We have a compatible pair.  Replace `prev` with the coalesced segment.  Clone to avoid\n            // modifying the segment instances currently in the MergeTree.\n            prev = prev.clone();\n            prev.append(segment.clone());\n          } else {\n            // The segment pair could not be coalesced.  Record the `prev` segment in the snapshot\n            // and make the current segment the new candidate for coalescing.\n            pushSeg(prev);\n            prev = segment;\n          }\n        } else {\n          // This segment needs to preserve it's metadata as it may be referenced by future ops.  It's ineligible\n          // for coalescing, so emit the 'prev' segment now (if any).\n          pushSeg(prev);\n          prev = undefined;\n          const raw = {\n            json: segment.toJSONObject()\n          };\n          // If the segment insertion is above the MSN, record the insertion merge info.\n          if (segment.seq > minSeq) {\n            raw.seq = segment.seq;\n            raw.client = mergeTree.getLongClientId(segment.clientId);\n          }\n          // We have already dispensed with removed segments below the MSN and removed segments with unassigned\n          // sequence numbers.  Any remaining removal info should be preserved.\n          if (segment.removedSeq !== undefined) {\n            assert(segment.removedSeq !== UnassignedSequenceNumber && segment.removedSeq > minSeq);\n            raw.removedSeq = segment.removedSeq;\n            raw.removedClient = mergeTree.getLongClientId(segment.removedClientId);\n          }\n          // Sanity check that we are preserving either the seq < minSeq or a removed segment's info.\n          assert(raw.seq !== undefined && raw.client !== undefined || raw.removedSeq !== undefined && raw.removedClient !== undefined);\n          // Record the segment with it's required metadata.\n          pushSegRaw(raw, segment.cachedLength);\n        }\n        return true;\n      };\n      mergeTree.walkAllSegments(mergeTree.root, extractSegment, this);\n      // If the last segment in the walk was coalescable, push it now.\n      pushSeg(prev);\n      return this.segments;\n    }\n    static loadChunk(storage, path, logger, options, serializer) {\n      return _asyncToGenerator(function* () {\n        const chunkAsString = yield storage.read(path);\n        return SnapshotV1.processChunk(path, chunkAsString, logger, options, serializer);\n      })();\n    }\n    static processChunk(path, chunk, logger, options, serializer) {\n      const utf8 = fromBase64ToUtf8(chunk);\n      const chunkObj = serializer ? serializer.parse(utf8) : JSON.parse(utf8);\n      return toLatestVersion(path, chunkObj, logger, options);\n    }\n  }\n  // Split snapshot into two entries - headers (small) and body (overflow) for faster loading initial content\n  // Please note that this number has no direct relationship to anything other than size of raw text (characters).\n  // As we produce json for the blob (and then encode into base64 and send over the wire compressed), this number\n  // is really hard to correlate with any actual metric that matters (like bytes over the wire).\n  // For test with small number of chunks it would be closer to blob size (before base64 encoding),\n  // for very chunky text, blob size can easily be 4x-8x of that number.\n  //# sourceMappingURL=snapshotV1.js.map\n  SnapshotV1.chunkSize = 10000;\n  return SnapshotV1;\n})();","map":null,"metadata":{},"sourceType":"module","externalDependencies":[]}